\section{Methodology}
Similarly to 
\cite{krauss2019statisticalArbitrage}
and \cite{krauss2016arbitrageSandP},
the methodology of this paper consists of
the following steps:

\begin{enumerate}
    \item The entire data set is split into training, validation and trading set
    \item The respective features (explanatory variables) and targets (dependent variables) are created
    \item Each model is trained on the training and validation set
    \item Conduct out-of-sample predictions and backtest trading on the trading set for each model 
    \item Evaluate its accuracy and trading-performance on the trading set respectively
    \item Go to Step 2, and repeat the same steps for a different feature and target specification
\end{enumerate}



%%% TRAINING AND TRADING SET
\subsection{Training and Trading Set} \label{ch:training_trading}
In this application to minute-binned data, the test set, i.e., trading set contains all observations from 01.11.2019 to 31.12.2019.
The training set ranges from 01.01.2019 to 14.09.2019, and the remaining 15.09.2019 to 31.10.2019 
is reserved for validation. I decided against the usual k-fold cross-validation approach 
in order to emphasize the importance of future observation for the model, since its performance only gets
evaluated on the future trading set. The daily cumulative short returns in the trading period of the top ten coins can be
seen below in figure \ref{fig:all_top_10_daily_cum_returns}. The returns in the trading period 
exhibit a downward trajectory across all coins, thus the positive short returns.

\begin{figure}[H] 
    \captionsetup{format=plain}
    \makebox[\textwidth]{\includegraphics[width=160mm]{all/all_top_10_daily_cum_returns.png}}
    \caption{
        This figure illustrates daily cumulative short returns in the trading period for the top ten coins selection.
    }
    \label{fig:all_top_10_daily_cum_returns}
\end{figure}


%%% FEATURE AND TARGET GENERATION
\subsection{Feature and Target Generation} \label{ch:feature_and_target}
Broadly following \cite{takeuchi2013momentumTrading},
the feature space gets generated as follows.

Let $ P^{c} = ( P^{c}_{t} )_{t \in T} $ denote the price process of coin/USD-pair $ c $, with $ c \in \{1, ... , n\} $. 
The price itself is defined as the average between \textit{Open} and \textit{Close}.

The following features and targets are generated from the data set:

% \begin{description}
\begin{basedescript}{%
    \desclabelstyle{\multilinelabel}
    \desclabelwidth{3cm}}    
    \item[Features:] 
    \begin{basedescript}{%
        \desclabelstyle{\multilinelabel}
        \desclabelwidth{2cm}}  
        \item[Returns:] Let $ R^{c}_{t, t - m} $ be the simple return for coin $ c $ over $ m $ periods defined as
        \begin{equation}
            \label{eq:return}
            R^{c}_{t, t - m} = \frac{ P^{c}_{t} }{ P^{c}_{t - m} } - 1 
        \end{equation} 
        \item[Volumes:] {
            Let $ V^{c}_{t} $ be the traded volume for coin $ c $ in minute-bin $ t $ scaled by Quantile-Transformer 
            fitted separately for each coin only on bins having traded volume. 
        }
    \end{basedescript}
    \item[Target:] {
        Let $ Y^{c}_{t + d, t} $ be a binary response variable for each coin $c$ 
        and the duration of the forecast interval $ d = 120, 240 $. 
        It assumes value 1 (class \textit{up}) if its future $d$ min return $ R^{c}_{t + d + 1, t + 1} $ is greater 
        than its cross-sectional median across all pairs $ ( R^{c}_{t + d + 1, t + 1} )^{C}_{c=1} $, 
        else -1 (class \textit{down}). 
        Instead of just using the simple return $ R^{c}_{t + d + 1, t + 1} $ as in \cite{krauss2019statisticalArbitrage},
        I included an additional condition, which demands that $  V^{c}_{t + d + 1} > 0 $ for realizing the feature return.
        If not skip bins until you reach a bin $ t^{*} $  in which $  V^{c}_{ t^{*} } > 0 $, 
        then realize return as in equation \ref{eq:return}.
    }
\end{basedescript}    
% \end{description}


The reason for this further restriction is to make the training of the model more similar to the 
trading decisions in the backtest, since only trades are allowed to be executed in a bin
if any volume was traded for the respective coin.
I decided for the inclusion of volume such that the model has a measure for 
taking trading activity into account without breaking vital assumptions needed for testing 
the 1. Efficient Market Hypothesis (\cite{malkiel2003marketHypothesis}, \cite{fama1970marketHypothesis}).
In addition, the volume got scaled for each coin in order to make the measure more comparable across coins,
since a single universal model is trained for each of the selected coins. 
Further, the Quantile-Transformation handles outliers and restricts the features to an interval ranging from 0 to 1.

Let $ \tau $ denote the transaction cost per trade in basis points (bps) proportional to the traded amount.
For sake of simplicity, assume that there is no differentiation between maker and taker transaction costs,
thus having equal transaction cost $ \tau = \tau_{taker} = \tau_{maker} $.
Incorporating costs $\tau$ leads to following expression for return $ R_{t, t-m}^{c} $ in long a position,

\begin{equation} \label{eq:return_transaction_cost_long}
    \begin{aligned}
        R_{t, t-m}^{c, long} &= \frac{ P_{t}^{c}(1 - \tau) - P_{t - m}^{c}(1 + \tau) }{ P_{t - m}^{c}(1 + \tau) }\\
        &= \frac{ P_{t}^{c} }{ P_{t - m}^{c} } \frac{ 1 - \tau }{ 1 + \tau } - 1,
    \end{aligned}
\end{equation}

and analogously for the short position,

\begin{equation} \label{eq:return_transaction_cost_short}
    \begin{aligned}
        R_{t, t-m}^{c, short} = \frac{ P_{t - m}^{c} }{ P_{t}^{c} } \frac{ 1 - \tau }{ 1 + \tau } - 1
    \end{aligned}
\end{equation}


%%% MODEL TRAINING
\subsection{Model Training}
As explained in chapter \ref{ch:training_trading}, I constructed a training set ranging from 01.01.2019 to 14.09.2019, 
a validation set ranging from 15.09.2019 to 31.10.2019, and a trading set ranging from 01.11.2019 to 31.12.2019.
Further, I restricted the training and validation set by excluding bins for which no volume 
was traded in the following bin or lagged values are not available.

I cross-validated the respective parameter space by first training the model
on the training set, and then evaluating it on the chronologically following validation set for each parameter combination.
After obtaining an accuracy evaluation of each combination, the model gets fitted with the hyperparameters of the best validation performance on
the combined training and valdation set.

Further, I also give a brief description of the models, and how I used them for the trading experiments.
Before doing so, I introduce the mathematical notation. 
Let $ D = \{ (x_{1}, y_{1}), ..., (x_{n}, y_{n}) \} $ denote the training data set containing $ n $ observations on which the respective model gets fitted
whereas $ x_{i} \in \mathbb{R}^{K} $ denotes the feature vector with $K$ features and $ y_{i} $ the target class variable for the $ i $-th observations respectively.
In addition, the Decision Tree is denoted \cite{breiman1984decisionTree} as a function $ T_{D}^{q}(x) $ with $ D $ denoting the set which it was trained on,
$ q $ the selection of features it was trained on and $ x $ feature vector as an the function argument.



%%%%%%%% LOGISTIC REGRESSION

\subsubsection{Logistic Regression} \label{ch:logistic_regression}
The Logistic Regression model analyzes the relationship between multiple independent variables or features and a
categorical dependent variable or target, and estimates the probability of occurrence
of an event by fitting data to a logistic curve. There are two models
of logistic regression, binary Logistic Regression and multinomial Logistic
Regression. Binary Logistic Regression is used in this application, since the dependent
variable is dichotomous (either \emph{down}- or \emph{up}-movement, i.e., 0 or 1),
whereas the multinomial one would be used to classify instances to multiple classes.

The building block for the Logistic Regression is the multiple regression model (\cite{james2013statisticalLearning}, \cite{park2013logisticRegression}),


\begin{equation}
    y = \sum_{j=1}^{J} \beta_{j} X_{j} + \epsilon =  X^{T}\beta + \epsilon
\end{equation}

with $y \in \mathbb{R}$ denoting the binary target, $\beta_{j}$ the parameter assigned to the $j$-th feature $X_{j}$ 
and $\epsilon$ the error term.
A problem that occurs when applying the multiple regression model is that the predicted values might
not fall between 0 and 1, thus not being eligible for estimating a probability $p(X)$.
When fitting a straight line to approximate a binary response coded as
0 or 1, it can occur that for some values of $X$ $p(X) < 0$ 
and $1 < p(X)$ for others (unless the range of X is limited) \cite{james2013statisticalLearning}.
Therefore, to avoid this problem in the Logistic Regression, 
the \emph{logistic function} gets wrapped around the multiple regression model,

\begin{equation} \label{eq:logistic_function}
    p(X, \beta) = \frac{ \exp( X^{T}\beta ) }{ 1 + \exp( X^{T}\beta ) } = \frac{ 1 }{ 1 + \exp( -X^{T}\beta ) }
\end{equation}

thus, the model can be written as,

\begin{equation}
    y = p(X, \beta) + \epsilon
\end{equation}


In order to estimate the coefficients $\beta$, the \emph{maximum likelihood} method can be used.
The objective is to find estimates for $\beta$
such that the predicted probability $p(x_{i}, \beta)$ of the price movement for each observed feature vector $x_{i}$,
using equation \ref{eq:logistic_function},
corresponds as closely as possible to the observed price movement $y_{i}$.
This objective can be formalized using a
mathematical equation called a \emph{likelihood function} \cite{james2013statisticalLearning}:

\begin{equation}
    \mathscr{L}(\beta) = \prod_{i=1}^{n} p(x_{i}, \beta)^{y_{i}} (1 - p(x_{i}, \beta))^{1 - y_{i}}
\end{equation}

Therefore, the optimization problem for the Logistic Regression takes the following form:

\begin{equation}
    \beta^{*} = \argmax\limits_{ \beta \in \mathbb{R}^{J} } \mathscr{L}(\beta)
\end{equation}

I decided to use the Logistic Regression for predicting price movements, 
because it is relatively easy to interpret and it was used extensively 
as a benchmark in the related literatur (\cite{fischer2017lstmMarketPrediction}, \cite{krauss2019statisticalArbitrage}). 
For this paper, I relied on the Scikit-learn implementation of \cite{sklearn2011} for the Logistic Regression
and follow the parameters outlined in \cite{fischer2017lstmMarketPrediction}. 

It seems reasonable to suppose that not only a few returns and volumes but most of them exhibit considerable explanatory effect.
Therefore, I chose the L2- over L1-regularization for the estimated parameters, 
which was also done in the related literatur (\cite{fischer2017lstmMarketPrediction}, \cite{krauss2019statisticalArbitrage}).
The optimal L2-regularization was
determined among 100 values on a logarithmic scale from 0.0001 to 10,000 via cross-validation as explained above
on the respective training set. Further, I restricted the maximum number of iterations to 150.


%%% RANDOM FOREST %%%
\subsubsection{Random Forest} \label{ch:random_forest}

Before outlining the algorithm for the Random Forest, I explain the essential 
building block of the Random Forest which is the Decision Tree.

The Decision Tree is a non-parametric machine learning method used for classification and regression.
It predicts the response with a set of if-then-else decision rules derived from the data.
The deeper the tree, the more complex the decision rules and the closer the model fits the data.
The Decision Tree builds classification or regression models in form of a tree structure.
Each node in the tree further partions the feature space into smaller and smaller subsets, 
while at the same time an associated Decision Tree is incrementally developed.
The final result is a tree with decision nodes and terminal nodes. 
A decision node has two or more branches.
Leaf nodes represent the actual classification or final decision. 
The topmost decision node in a tree which corresponds to the best predictor is called the root node.
Decision Trees can handle both categorical and numerical data \cite{james2013statisticalLearning}.

An example of such a tree is depicted below in figure \ref{fig:decision_tree_example}.

\begin{figure}[H]
    \captionsetup{format=plain}
    \makebox[\textwidth]{\includegraphics[width=100mm]{forest/decision_tree_example.png}}
    \caption{Given a data set with two features height and weight, and gender as the target variable, 
             this example tree stratifies the two-dimensional feature space into three distinct subset each 
             represented by the terminal nodes at the bottom.
             The stratification occurs at the two deciding nodes depending either on whether its height is above 180 cm 
             and its weight is above 80kg.
             }
    \label{fig:decision_tree_example}
\end{figure}

In the following, I describe the CART algorithm for tree building as specified in \cite{breiman1984decisionTree}.
The basic idea of tree growing is to choose a split among all the possible splits at each node
such that the resulting child nodes are the "purest". In this algorithm, only univariate splits are
considered. That is, each split depends on the value of only one predictor variable. All
possible splits consist of possible splits of each predictor.

A tree is grown starting from the root node by repeatedly using the following steps on each
node in the following algorithm:

\begin{algorithm}[H]
    \caption{Binary Splitting \cite{breiman1984decisionTree}}
    \label{alg:tree_binary_splitting}
    \KwData{ Training set $D$ with $J$ features }
    \KwResult{ Decision Tree $ T_{D}^{q=J} $ }

    Initialize \emph{DecisionTree}  (i.e. $ T_{D}^{q=J} $)


    \While{ DecisionTree.checkStoppingCriterion() }{

        \tcc{ Loop over nodes associated with decision rules of $ T_{D}^{q=J} $ }

        \For{ $ t \in  DecisionTree.nodes $ }{

            \begin{itemize}
                \item[(i)] \textbf{Find best split \(s\) for each feature \(X_{j}\):}
                For each feature \(X_{j}\), there exist \(J-1\) potiential splits, 
                whereas \(J\) is the number of different values for the respective feature.
                Evaluate each value \(X_{i,j}\) at the current node \(t\) as a 
                candidate split point (for \(x \in X_{j}\), if \(x \leq X_{k,i}=s\),
                then \(x\) goes to left child node \(t_{L}\) else to right child node \(t_{R}\)).
                The best split point is the one that maximize the splitting criterion $\Delta i(t, t_{L}, t_{R})$ the most when the node is split according to it.

                $ j^{*}, s^{*} \leftarrow \argmax\limits_{ j \in \{ 1, ..., J \}, s_{j} \in \{ x_{j}: x_{j}  \in t \} }  \Delta i(t, t.split(s_{j}, j))  $

        
                \item[(ii)] \textbf{Add decision rule associated with split $s^{*}$, feature $x_{j^{*}}$ and node $t$ to decision tree $T_{D}^{q=J}$ } 

                $ DecisionTree.splitNode(t, s^{*}, j^{*})$
    
            \end{itemize}
        }
    }

\end{algorithm}


The splitting criterion $ \Delta i(t) $ is defined as, 

\begin{equation}
    \Delta i(t, t_{L}, t_{R}) = i(t) - p_{L} i(t_{L}) - p_{R} i(t_{R}),
\end{equation}

with the Gini Coefficient being used as the impurity measure $i(t)$ in the CART algorithm,

\begin{equation}
    i(t) = \sum_{c \in C} p(c|t) (1 - p(c|t))
\end{equation}

However, there exist variants of the algorithm which incroporate different impurity measures such as
Information Gain or Variance Reduction \cite{rutkowski2011cart}.

Since the Decision Tree tends to overfit the data (\cite{kerdprasop2011discreteDecisionTree}, \cite{geman1992biasVariance}),
especially in the case of large Decision Trees,
the Random Forest was introduced to mitigate this problem. This is done by casting a vote based on an 
ensemble of individual Decision Trees trained on bootstrapped samples from data $D$. 
Further, in order to decorrelate the Decision Trees from each other, 
only a random subset of features is considered at step $(i)$ of algorithm \ref{alg:tree_binary_splitting} 
each time a node is split further, i.e., an additional decision rule gets added.
This process is further illustrated in algorithm \ref{alg:rf_ensemble_growth} below.

\begin{algorithm}[H]
    \caption{ Generation of Decision Tree ensemble for the Random Forest \ref{alg:rf_ensemble_growth} }
    \KwData{ Training set $D$, ensemble size $n_{E}$  }
    \KwResult{ Tree ensemble $E = \{ T_{ D_{1} }^{q}, ..., T_{ D_{ n_{E} } }^{q} \}  $ }
    \tcc{ Generate $n_{E}$ Decision Trees }
    \For{ $i \in \{ 1, ..., n_{E} \}$}{
        \tcc{ Draw bootstrapped sample $ D_{i} $  from $D$ }

        $ D_{i} \leftarrow bootstrap(D) $

        \tcc{ 
            Grow Decision Tree as specified in algorithm \ref{alg:tree_binary_splitting}, 
            except only use a randomly selected feature subset of size $q$ in step $(i)$
        }
        
        $ tree \leftarrow DecisionTree.fit(  D_{i} , features\_selection=q) $

        $ E \leftarrow E \cup tree $
    }
    \label{alg:rf_ensemble_growth}
\end{algorithm}

The reason that the Random Forest might be a suitable candidate for predicting price movements 
lies in its ability to model complex non-linear decision boundaries and to grow arbitrarily large models.
In addition, Random Forests in this configuration are the best method in similar works as in \cite{krauss2016arbitrageSandP}
and the method of choice machine learning application
on monthly stock market data in \cite{moritz2014partFutureReturns}. Thus, Random Forests can be seen as a viable benchmark for 
experimental setups such as this one.

In this application of the Random Forest, I conducted a cross-validated grid-search over the number of Decision Trees in the
ensemble $n_{E} \in \{100, 500, 1000\}$ 
and the maximum depth of each tree $ d_{ T_{ D_{i} }^{q} } \in \{3, 5, 10, 15 \} $ 
taking advice from \cite{krauss2016arbitrageSandP}, \cite{krauss2019statisticalArbitrage} and \cite{sklearn2011}.


%%% ADABOOST %%%
\subsubsection{AdaBoost} \label{ch:adaboost} 

The Adaptive Boosting (AdaBoost) is a classification machine learning algorithm that combines multiple so called weak learners.
First, it fits weak classifiers,
a Decision Tree in this application, on the original dataset.
Then, it proceeds to fit additional weak classification models on the dataset for which 
weights of previously incorrectly classified instances are adjusted 
such that the following classifiers focus more on difficult cases \cite{schapire1997adaboost}.
According to \cite{hastie2009multiClassAdaboost}, under zero-loss, the missclassification rate is given by $ 1 - \sum_{k=1}^{K} E_{x} \mathds{1}_{ C_(X)=k } P(C=k | X)  $.
Therefore, the \emph{Bayes Classifier}

\begin{equation}
    C^{*}(x) = \argmax\limits_{k} P(C = k| X = x)
\end{equation}

will minimizes this quantity with the missclassification error rate, i.e., \emph{Bayes Error Rate},
equal to $ 1 - E_{X} max_{k} P(C = k | X) $. The objective of the AdaBoost algorithm is to iteratively 
approximate the Bayes Classifier $C^{*}(x)$ by combining \emph{weak learners} \cite{hastie2009multiClassAdaboost}.
These are classification models wich have only a small predictive power.
In this application, I use small Decision Trees with a depth of up to three. 
First, the AdaBoost fits a classifier with the unweighted training
sample that assigns class labels to each instance in the training data set. 
If the respective assigned label does not equal the true one,
the weight of that training data instance is increased. 
Then, the AdaBoost proceeds to train a second classifier including
the new weights. 
For instances that got misclassified, the weights get boosted, and the procedure gets repeated.

The AdaBoost algorithm is illustrated below:

\begin{algorithm}[H] \label{alg:adaboost}
    \caption{AdaBoost \cite{hastie2009multiClassAdaboost}, \cite{schapire1997adaboost} }
    
    \KwData{ Training set $D$ with $K$ features }

    \KwResult{ $AdaBoost(x)$ } 
    
    Initialize weights $w_{i} = 1/n$ for $i = 1, ..., n$

    Initialize DecisionTree

    \For{ $ m \in \{ 1, ..., M \} $ }{

        \tcc{ Apply weights to the data set }

        $ \widetilde{D} $ = adjustDataSet($D$, $ \{ w_{i} \}_{i=1}^{n} $)

        \tcc{ Fit m-th classifier }

        $ T_{ \widetilde{D} }^{(m)} \leftarrow $ DecisionTree.fit($\widetilde{D}$)

        \tcc{ Compute weighted error rate  }

        $ err^{m} \leftarrow \sum_{i=1}^{n} w_{i} \cdot \mathds{1}( c_{i} \neq T_{ \widetilde{D} }^{ (m) } (x_{i}) ) / \sum_{i=1}^{n} w_{i} $

        \tcc{ Compute $\alpha$ parameter }

        $ \alpha^{(m)} \leftarrow ln \frac{ 1 - err^{m} }{ err^{m} } $

        \tcc{ Readjust weights }

        $ w_{i} \leftarrow w_{i} \cdot \exp\biggl( \alpha^{(m)} \cdot \mathds{1}( c_{i} \neq T_{ \widetilde{D} }^{ (m) } (x_{i}) ) \biggr),\, for \; i=1, ..., n $

        \tcc{ Renormalize weights }

        $ \{ w_{i} \}_{i=1}^{n} \leftarrow $ renormalize( $ \{ w_{i} \}_{i=1}^{n} $ )

    }

    \Return $ C(x) = \argmax\limits_{k} \sum_{m=1}^{M} \alpha^{(m)} \cdot \mathds{1}( T_{ \widetilde{D} }^{ (m) } (x_{i}) = k ) $ 


\end{algorithm}

According to \cite{schapire1999boostingIntro}, the AdaBoost is relatively fast, 
and has a low amount of parameters to tune. It requires no prior knowledge about
the weak learner and so can be flexibly combined with any method for finding weak hypotheses.
Further, the AdaBoost is supposed to be a good candidate for to identifying outliers, because
it focuses its weight on the hardest examples, the examples with the highest weight often
turn out to be outliers (\cite{schapire1996boostingExperiments}, \cite{schapire1999boostingIntro}).
This property could be highly valuable when trying to predict price movements, 
since these outliers could exhibit the most reliable probability signals for the trading algorithm.
On the other hand, boosting can fail to perform well given insufficient data and 
seems to be especially susceptible to noise (\cite{schapire1999boostingIntro}, \cite{dietterich2000ensembleComparison}).
Therefore, this experiment shows if and how this tradeoff will be resolved. 
In this application of the AdaBoost, I conducted a cross-validated grid-search over the number of weak learners, i.e., Decision Trees,
$M \in \{500, 1000\}$,
the maximum depth of each tree $ d_{ T^{ (m) } } \in \{1, 3\} $,
and the learning rate $ \lambda \in \{ 0.001, 0.01, 0.1 \}$
taking advice from \cite{krauss2016arbitrageSandP}, \cite{hastie2009statisticalLearning} and \cite{friedman2002gradientBoosting}.



%%%% DEEP NEURAL NETWORK %%%%
\subsubsection{Deep Neural Network}
In this chapter, I briefly describe the Deep Neural Network (DNN) according to \cite{dixon2015annMarketPrediction} and \cite{krauss2016arbitrageSandP}. 
A deep neural network consists of an input layer, one or more hidden layers, and an output layer.
The topology of the network can be seen in figure \ref{fig:deep_network_topology}. 

\begin{figure}[H]
    \captionsetup{format=plain}
    \makebox[\textwidth]{\includegraphics[width=150mm]{ann/deep_network_topology.png} }
    \caption{ 
        This figure illustrates the topology of a deep neural network consisting of two hidden layers, seven features and two output states \cite{dixon2015annMarketPrediction}.
        }
    \label{fig:deep_network_topology}
\end{figure}

The input layer matches the feature space in dimensions, thus there are
as many input neurons as predictors. The output layer is either a classification or regression 
layer to match the output space. All layers consist of neurons, which are themselve connected to the ones from the neighboring layers. 
Thus, each neuron in layer $ l $  is 
connected to all neurons in the following layer $ l + 1 $. Further, each neuron in a input and hidden layer has a bias parameter $b_{j}^{l}$ similar to actual biological neurons.
Therefore, each neuron $j$ of layer $l$ receives a weighted combination $ \alpha_{j}^{l}  $ of the
$ n_{l - 1} $ outputs of the neurons $i = 1, ..., n_{l - 1} $ in the previous layer $ l - 1 $ as input,

\begin{equation}
    \alpha_{j}^{l} = \sum_{ i=1 }^{ n_{l - 1} } w_{i, j}^{l} x_{i}^{l - 1} + b_{j}^{l},
    \label{eq:dnn_alpha_weighted_combination}
\end{equation}

with $ w_{i, j}^{l} $ denoting the corresponding weight in layer $l$ for the input $ \alpha_{j}^{l} $ of the previous layer $l - 1$.
$ b_{j}^{l} $ denotes the bias for neuron $j$ of layer $l$. 
The weighted combination $ \alpha $ of equation \ref{eq:dnn_alpha_weighted_combination} is transformed by
a so-called activation function $ f $, such that the output signal $ f(\alpha) $ is forwarded to the neurons
in layer $ l + 1 $. Comparing equation \ref{eq:dnn_alpha_weighted_combination} and the Logistic Regression from chapter \ref{ch:logistic_regression}, 
the similarities become apparent. In that sense, a DNN can be interpretet as multiple sequences of Logistic Regression
chained together with different activation functions converging into the final model output.
The activation function used in this application is ReLU (rectified linear activation unit) (\cite{arora2018relu}, \cite{berner2019reluDerivative})
$ f: \mathbb{R} \rightarrow \mathbb{R} $,

\begin{equation}
    f( \alpha^{l} ) = max( \alpha^{l}, 0 ) = ( max\{0, \alpha^{l}_{1}\}, ..., max\{0, \alpha^{l}_{  n_{l}  }\} )^{T}
    \label{eq:relu}
\end{equation}

The choice for this activation function is inspired by the sudden burst of action potential releases of biological neurons \cite{zhang2019actionPotential}.

Let $ W = \bigcup_{l=1}^{L - 1} W_{l} $, with $W_{l} = (w_{i, j}^{l}) \in \mathbb{R}^{ n_{l - 1} \times n_{l} } $ denote the
weight matrix that parametrizes layers $l$ and $l + 1$ for a network consisting of $L$ layers. 
Further, let the set $ B = \bigcup_{l=1}^{L - 1} b_{l}  $,
with $ b_{l} = ( b_{j}^{l} ) \in \mathbb{R}^{ n_{l - 1} } $ denote the vector of biases for layer $l$ such that the objective is to find
the optimal $ \theta = (W, B) $ parameters.
The DNN is optimized by adapting weights $ \theta $ in order to minimize the error on the training data.
This is done by minimizing the loss function $\mathscr{L} (\theta)$ for each training example $(x_{i}, y_{i}) \in D$. 
Since this experimental setup consists of a classification problem, the loss function is the cross-entropy,

\begin{equation}
    \mathscr{L} (\theta) = - \sum_{i=1}^{n} \sum_{ k=1 }^{K}  ln( m(y=y_{k} | x_{i}; \theta))  \cdot y_{k},
    \label{eq:dnn_loss_function}
\end{equation}

with $ m(y=y| x_{i}) $ denoting the DNN fully parametrized by $ \theta $  as a function of an observed feature vector $x_{i}$. 
Since the softmax function \cite{dixon2015annMarketPrediction} is used to transform the output from the last hidden layer,
the DNN can expressed as follows:

\begin{equation}
    m( y=y_{k} | x_{i} ) =  \frac{ \alpha_{c}^{L} }{ \sum_{j=1}^{K} \alpha_{c}^{L} }
\end{equation}

The loss function \ref{eq:dnn_loss_function} is minimized
by stochastic gradient descent, with the gradient of the loss function $\Delta_{\theta} \mathscr{L} (\theta)$
being calculated via backpropagation \cite{krauss2016arbitrageSandP}.


\begin{equation}
    f^{'}(\alpha^{l}) = ( \mathbb{I}(\alpha^{l}_{1} > 0), ..., \mathbb{I}(\alpha^{l}_{ n_{l} } > 0) )^{T}
\end{equation}

Following \cite{rojas1996neuralNetworks}, \cite{ruder2016gradientDescent}, and \cite{dixon2015annMarketPrediction},
the backpropagation learning algorithm based on the method of stochastic gradient descent (SGD) updates 
the parameters $ \theta $ after randomly drawing an observation $i$,

\begin{equation}
    \theta = \theta - \lambda \nabla_{ \theta } \mathscr{L}( \theta, i )
\end{equation}

with $\lambda$ denoting the learning rate.
Algorithm \ref{alg:stochastic_gradient_descent} below provides a high level pseudo-algorithmic description of the sequential version of
SGD according to \cite{dixon2015annMarketPrediction}.

\begin{algorithm}[H]
    \caption{ Stochastic Gradient Descent \cite{dixon2015annMarketPrediction} }

    \KwData{ Training set $D$ features and parameter space of DNN }

    \KwResult{ Optimized parameter values $\theta$ for DNN } 

    \tcc{ Draw initial parameter values from normal distribution }

    $ \theta \leftarrow r $ with $ r = (r_{i})_{i=1}^{n}  $ and $ r_{i} \in \mathscr{N}( \mu, \sigma ), \forall i $

    \tcc{ Get loss according to initial parametrisation }

    $ \mathscr{L} \leftarrow 0 $

    \For{ $  i \in \{ 1, ..., n \} $ }{
        $ \mathscr{L} \leftarrow \mathscr{L} + \mathscr{L}_{i}( \theta ) $
    }

    \tcc{ As long as loss above threshold $\tau$, continue to update $\theta$ }

    \While{ $ \mathscr{L} \geq \tau  $  }{

        \For{ $  i \in \{ 1, ..., n \} $ }{

            \tcc{ Draw random sample from data $D$ }

            $ i \leftarrow drawRamdomly(D) $

            $ \theta \leftarrow \theta - \lambda \nabla_{ \theta } \mathscr{L}( \theta, i ) $
        }

        $ \mathscr{L} \leftarrow 0 $

        \For{ $  i \in \{ 1, ..., n \} $ }{
            $ \mathscr{L} \leftarrow \mathscr{L} + \mathscr{L}_{i}( \theta ) $
        }
    }
    \label{alg:stochastic_gradient_descent}
\end{algorithm}


In order to describe the network's topology, the following notation gets used: I-H1-H2-H3-H4-O.
I denotes the number of input neurons, H1, H2, H3 and H4 the number of hidden neurons in hidden layers 1, 2, 3, and O the number of output
neurons. According to \cite{krauss2016arbitrageSandP}, a popular rule,  which I also abide by, is to set the number of neurons in the first hidden layer H1 of a DNN
to the number of inputs.
To introduce a closer fit to the data, I also set H1 = H2.
A bottleneck gets introduced by setting H3 = 15 and H4 = 10, thus enforcing a reduction in dimensionality in line with \cite{takeuchi2013momentumTrading} and 
\cite{dixon2015annMarketPrediction}.
Let $K$ be the number of features of the data. Then, the topology specification results in 
I-H1-H2-H3-O = $K$-$K$-$K$-15-10-2 with input layer I matching the number of features.
The model got trained across 400 epochs, i.e., the data gets passed 400 times over the training set, as in \cite{huck2009pairSelection}.

As a disclaimer, I would like to point out that according to \cite{zhang1998annStateOfTheArt}, the design of a DNN is more of an art than a science.
Therefore, I mainly followed best practices and intuitions to calibrate the DNN. 
However, for the purpose of this paper, it can be argued that it is sufficient, since it takes the operational costs aspect of such an endeavour into account.
The actual reason for using the DNN is its ability to model complex decision boundaries \cite{karimi2019DNNdecisionBoundary}. 
This flexibility could be of great use for predicting price movements.



\subsection{Trading Algorithm} \label{ch:trading_algorithm}
For the trading phase, I proceeded similarly to 
\cite{krauss2019statisticalArbitrage}, \cite{gatev2006pairsTrading} and \cite{krauss2016arbitrageSandP}.
After having trained each model for different specifications, probability estimates got generated for each class
based on the trading sets feature's. These probabilities for each bin were then used as signals in bin $t$ for each coin
in order for the algorithm to decide whether to enter or close a position invested in a coin in the next bin in $ t + 1 $.
In this work, I refer to these positions as active.
More specifically, the algorithm compares the probability estimate for each coin and decides 
whether to close the current position and enter a new one. The coin with the highest probability estimate for a down movement
is considered as a candidate for the short position if its probability is also above a certain threshold,
since it is most likely to go down. 
The algorithm proceeds analogously for the long position. 
If a coin gets chosen this way and the position for this movement is active, then it proceeds to close this position.
Therefore, a long and a short position gets opened at most per bin $ t $.
In order get a better estimate of the return, I entered 60 initial short and long positions at different points in time,
thus the resulting portfolio has 120 active positions at most simultaneously.
Then, I proceeded to calculate aggregate values for each of these positions. To render the backtest more
realistic, I incorporated the following constraints:
\begin{basedescript}{%
    \desclabelstyle{\multilinelabel}
    \desclabelwidth{4,5cm}}
    \item[Minimum duration:] {
        Any active position has a minimum duration of $ d = 120, 240 $ before considering closing it.
    }
    \item[Execution gap:] {
        To account for the time it takes to generate a probability signal and submit the order accordingly,
        I introduce an execution gap of one minute. This means that when generating the signal from a bin in $ t $,
        the order gets executed in $ t + 1 $ the earliest.
    }
    \item[Minimum volume:] { 
        Orders for opening or closing a position only get executed in bin $ t $
        if any volume was traded in the respective bin.
    }
    \item[Order cancel:] {
        If after submitting the order in $ t $ the traded volume in bin $ t + 1 $
        is zero, the order gets canceled, and a new probability signal gets generated in bin $ t + 1 $.
    }
    \item[Transaction cost:] {
        For every order execution a transaction cost of $ \epsilon $ bps gets subtracted. 
        Thus, the opening and closing of a position costs $ 2 \times \epsilon $ bps.
    }
    \item[Keep active position:] {
        If the probability signal yields the same coin as the one from the current active position,
        then keep the same position open for another $ d=120, 240 $ minutes before again generating another
        probability signal.
    }
\end{basedescript}


The algorithm described above can also be expressed in a pseudo-algorithmic way:

\begin{algorithm}[H]
    \caption{Pseudo-Algorithm for a single position ( short and long are analogous) }
    \KwData{ Minute-binned Test-OHLC-Data $OHLC$ and its estimated probabilities }
    \KwResult{ Trading decisions and its realized returns }

    Initiate \emph{Position}

    $row\_number \leftarrow 0 $

    \While{ $ row\_number < (max\_row\_number - delta) $ }{
        \tcc{ Load row data given by $ row\_number $ }

        $row \leftarrow OHLC.getRow( row\_number ) $

        \tcc{ Obtain maximum probability pair $max\_prob\_pair$ } 

        $max\_prob\_pair \leftarrow row.getMaxProbPair() $

        \tcc{ Determine whether probability condition is fulfilled }

        \uIf{ row.getProb( max\_prob\_pair ) $>$ threshold }{
            \uIf{ Position.active == False  }{
                Position.open()
            }
            \uElseIf{  max\_prob\_pair != Position.pair }{
                Position.close()
            }
        }
        \Else{
            \uIf{ Position.active == True }{
                Position.close()
            }
        }

        \tcc{ Skip minute bins based on current position }

        \uIf{ Position.active == True }{
            $ row\_number \gets row\_number + delta $
        }
        \Else{
            $ row\_number \gets row\_number + 1 $
        }
    }

    
   \end{algorithm}
   